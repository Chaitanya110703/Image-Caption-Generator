{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c4fdc-6428-4b31-9406-017615620617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672eb34-0f3a-4675-9b4f-9ed09e19de5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORKING_DIR = 'working'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41b98f-8dd3-4683-a517-1163ba1540f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2253ac89-45ec-4566-bbae-e9c534525b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48716a4-d0f5-48c8-8ce1-5dc98f2d405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_features = nn.Sequential(*list(vgg16.children())[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d3070c-ee12-4798-8c8d-c2966ee62a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_features = vgg16_features.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c56dd44-4afd-4012-8223-0d9f46b61319",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb555b1-0793-4d50-9f61-5591f2dec51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to 224x224\n",
    "        transforms.ToTensor(),         # Convert image to PyTorch tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "    image = Image.open(image_path).convert('RGB')  # Ensure 3 channels (RGB)\n",
    "    return preprocess(image).unsqueeze(0).to(device)  # Add batch dimension and move to device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693ddd3-91bc-44db-8e35-4b799baf6891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(captions):\n",
    "    # Ensure captions is a list of strings\n",
    "    if not isinstance(captions, list) or not all(isinstance(c, str) for c in captions):\n",
    "        raise ValueError(\"Input captions must be a list of strings.\")\n",
    "\n",
    "    # Tokenize the captions\n",
    "    encoded = tokenizer(\n",
    "        captions,\n",
    "        padding=True,          # Pad sequences to the same length\n",
    "        truncation=True,       # Truncate sequences that are too long\n",
    "        return_tensors=\"pt\",   # Return PyTorch tensors\n",
    "    )\n",
    "    \n",
    "    # Return the 'input_ids' tensor\n",
    "    return encoded[\"input_ids\"].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864eeeb8-ddee-4110-a017-7857956faf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sequences(tokenized_captions, vocab, max_length):\n",
    "    # Convert tokens to indices\n",
    "    sequences = [[vocab[token] for token in caption if token in vocab] for caption in tokenized_captions]\n",
    "    # Pad sequences to the maximum length\n",
    "    padded_sequences = pad_sequence([torch.tensor(seq) for seq in sequences], \n",
    "                                     batch_first=True, \n",
    "                                     padding_value=vocab['<pad>']).to(device)\n",
    "    return padded_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4f0bb9-eb5b-4034-94f0-ae4c869b7a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\"The cat sits on the mat.\", \"A dog barks loudly.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6d267b-77cf-43d7-8023-ad6767df575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_captions = tokenize_text(captions)\n",
    "print(\"Tokenized Captions:\", tokenized_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c531c306-f6c0-41c8-9205-4a7151e0f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = process_sequences(tokenized_captions, vocab, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04270de9-1c7a-4717-a2a2-cd2645cf759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Padded sequences:\", padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85e7568-6f66-43de-b841-f1650b264ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(CaptionGenerator, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        captions = self.embedding(captions)\n",
    "        inputs = torch.cat((features.unsqueeze(1), captions), dim=1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85461c93-08cd-4d49-94b1-b5ba659ecfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize caption generator\n",
    "vocab_size = len(vocab)\n",
    "embed_dim = 256\n",
    "hidden_dim = 512\n",
    "caption_model = CaptionGenerator(vocab_size, embed_dim, hidden_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf76df8-37de-4622-912e-a61aaeb53224",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Caption Generator Model:\", caption_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e51e1fa-6bcf-448b-8569-0863ed1cddf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_path):\n",
    "    # Preprocess image\n",
    "    image_tensor = preprocess_image(image_path)\n",
    "    \n",
    "    # Extract features using VGG16\n",
    "    with torch.no_grad():\n",
    "        features = vgg16_features(image_tensor).squeeze()\n",
    "    \n",
    "    # Dummy captions for testing (replace with actual captions for real use)\n",
    "    captions = torch.tensor([[1, 3, 0, 0, 0]]).to(device)  # Example input sequence\n",
    "    \n",
    "    # Generate caption\n",
    "    outputs = caption_model(features, captions)\n",
    "    print(\"Generated output:\", outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dcec28-5873-4406-a796-d2cb3bbd0bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_dir, captions_file, tokenizer, transform=None, max_length=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_dir (str): Directory with all the images.\n",
    "            captions_file (str): Path to the file containing image-caption pairs.\n",
    "            tokenizer (object): Tokenizer to process the captions.\n",
    "            transform (callable, optional): Transformations for images.\n",
    "            max_length (int): Maximum length for tokenized captions.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # Load image-caption pairs\n",
    "        with open(captions_file, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "            self.image_caption_pairs = [line.strip().split(\"\\t\") for line in lines]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_caption_pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, caption = self.image_caption_pairs[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        \n",
    "        # Load and transform image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # Tokenize caption\n",
    "        encoded_caption = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return image, encoded_caption[\"input_ids\"].squeeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0790896e-838f-4f06-aa36-fd538658fef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "image_dir = working  \n",
    "captions_file = \"path_to_captions.txt\" \n",
    "# Create dataset\n",
    "dataset = ImageCaptionDataset(\n",
    "    image_dir=image_dir,\n",
    "    captions_file=captions_file,\n",
    "    tokenizer=tokenizer,   \n",
    "    transform=transform,\n",
    "    max_length=50         \n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,          \n",
    "    shuffle=True,\n",
    "    num_workers=4,          \n",
    "    pin_memory=True         s\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53a2dd6-b63e-4ff9-9bd4-c41a82a990fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(caption_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    caption_model.train()\n",
    "    epoch_loss = 0\n",
    "    for images, captions in dataloader:  # Replace `dataloader` with your actual DataLoader\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Extract features\n",
    "        features = vgg16_features(images).view(images.size(0), -1)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = caption_model(features, captions[:, :-1])  # Input captions excluding the last token\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions[:, 1:].reshape(-1))  # Target captions excluding the first token\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff92c154-fd3a-4680-9a4a-37a37d210d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Saving the Model\n",
    "def save_model(model, optimizer, epoch, path=\"model_checkpoint.pth\"):\n",
    "    \n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }, path)\n",
    "    print(f\"Model saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db4a26-c586-4ff9-a1c2-1c04308c2844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "def load_model(model, optimizer, path=\"model_checkpoint.pth\", device=\"cpu\"):\n",
    "    \n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    print(f\"Model loaded from {path}, resuming from epoch {epoch}\")\n",
    "    return epoch\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
